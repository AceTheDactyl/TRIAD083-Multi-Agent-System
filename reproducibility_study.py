#!/usr/bin/env python3
"""
Reproducibility Study Framework
================================

Scientific validation of the extraordinary overperformance signal (z~15.0).

Research Questions:
1. Is the 60% burden reduction reproducible? (predicted: 15.3%)
2. Does it occur consistently at z‚âà0.867 (critical point)?
3. What is the variance across independent trials?
4. Is R1‚ÜíR2‚ÜíR3 amplification stable?
5. Does performance scale or saturate near critical point?

Methodology:
- Systematic z-range scanning: z ‚àà [0.85, 0.90] in 0.01 steps
- Multiple trials per z-value (n=20 minimum for statistical power)
- Controlled conditions: fixed burden profiles, consistent evolution
- Statistical validation: z-scores, p-values, confidence intervals
- Null hypothesis: Burden reduction = 15.3% ¬± random variation

Success Criteria:
- p < 0.001 for reproducibility (99.9% confidence)
- Coefficient of variation < 20% (stable results)
- Effect size > 2.0 standard deviations
- R1‚ÜíR2‚ÜíR3 amplification consistent within 10%
"""

import json
import math
import statistics
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

from unified_sovereignty_system import (
    UnifiedSovereigntySystem,
    create_demo_burden,
    evolve_cascade_state
)
from unified_cascade_mathematics_core import (
    UnifiedCascadeFramework,
    CascadeSystemState
)
from phase_aware_burden_tracker import BurdenMeasurement


@dataclass
class TrialResult:
    """Result from a single experimental trial."""
    trial_id: int
    z_target: float
    z_actual: float
    initial_burden: float
    final_burden: float
    burden_reduction_pct: float
    cascade_amplification: float
    r1_to_r2_ratio: float
    r2_to_r3_ratio: float
    snapshots_count: int
    phase_transitions: List[str]


@dataclass
class ZRangeAnalysis:
    """Statistical analysis for a specific z-value."""
    z_value: float
    trials_count: int
    mean_reduction: float
    std_reduction: float
    min_reduction: float
    max_reduction: float
    coefficient_variation: float
    z_score: float  # Relative to predicted 15.3%
    p_value: float
    confidence_interval_95: Tuple[float, float]
    is_significant: bool  # p < 0.001
    mean_amplification: float
    std_amplification: float


@dataclass
class ReproducibilityReport:
    """Complete reproducibility study report."""
    study_id: str
    timestamp: str
    z_range: Tuple[float, float]
    z_step: float
    trials_per_z: int
    total_trials: int
    predicted_reduction: float  # 15.3%

    # Overall findings
    overall_mean_reduction: float
    overall_std_reduction: float
    overall_z_score: float
    overall_p_value: float

    # Per-z analysis
    z_analyses: List[ZRangeAnalysis]

    # Key findings
    is_reproducible: bool
    optimal_z_value: float
    optimal_reduction: float
    emergence_detected: bool
    artifact_suspected: bool

    # Recommendations
    recommendations: List[str]


class ReproducibilityStudy:
    """
    Systematic reproducibility testing framework.

    Tests cascade system behavior across critical z-range to validate
    the extraordinary 60% burden reduction claim (vs 15.3% predicted).
    """

    def __init__(self,
                 z_min: float = 0.85,
                 z_max: float = 0.90,
                 z_step: float = 0.01,
                 trials_per_z: int = 20):
        """
        Initialize reproducibility study.

        Args:
            z_min: Minimum z-value to test (default: 0.85)
            z_max: Maximum z-value to test (default: 0.90)
            z_step: Step size for z-scanning (default: 0.01)
            trials_per_z: Number of trials per z-value (default: 20)
        """
        self.z_min = z_min
        self.z_max = z_max
        self.z_step = z_step
        self.trials_per_z = trials_per_z

        self.predicted_reduction = 15.3  # From original analysis

        self.trial_results: List[TrialResult] = []

    def run_single_trial(self,
                        z_target: float,
                        trial_id: int,
                        evolution_steps: int = 10) -> TrialResult:
        """
        Run a single controlled trial evolving toward target z-value.

        Args:
            z_target: Target z-coordinate to evolve toward
            trial_id: Unique trial identifier
            evolution_steps: Number of evolution steps (default: 10)

        Returns:
            TrialResult with measurements
        """
        system = UnifiedSovereigntySystem()
        framework = UnifiedCascadeFramework()

        # Start from LOW sovereignty baseline (subcritical_early phase)
        # This ensures we can measure burden reduction during evolution
        initial_clarity = 0.35
        initial_immunity = 0.30
        initial_efficiency = 0.32
        initial_autonomy = 0.28

        state = framework.compute_full_state(
            clarity=initial_clarity,
            immunity=initial_immunity,
            efficiency=initial_efficiency,
            autonomy=initial_autonomy
        )

        initial_z = state.z_coordinate

        # Calculate deltas needed to reach target z in evolution_steps
        # z_target determines how much sovereignty we need to gain
        # Use scaling to reach desired z-value
        target_scale = z_target / 0.867  # 0.867 is critical point reference

        target_clarity = 6.18 * target_scale
        target_immunity = 8.67 * target_scale
        target_efficiency = 7.50 * target_scale
        target_autonomy = 9.0 * target_scale

        # Compute per-step deltas
        clarity_delta = (target_clarity - initial_clarity) / evolution_steps
        immunity_delta = (target_immunity - initial_immunity) / evolution_steps
        efficiency_delta = (target_efficiency - initial_efficiency) / evolution_steps
        autonomy_delta = (target_autonomy - initial_autonomy) / evolution_steps

        # Record actual z achieved (will update at end)
        z_actual = initial_z

        # Create initial burden profile
        initial_burden = create_demo_burden(state.phase_regime)
        initial_weighted = self._compute_weighted_burden(initial_burden)

        # Capture initial snapshot
        system.capture_snapshot(state, initial_burden, include_advanced_analysis=False)

        # Record phase transitions
        phase_transitions = [state.phase_regime]

        # Evolve system toward target z-value
        for step in range(evolution_steps):
            # Evolve cascade state using calculated deltas
            state = evolve_cascade_state(
                state,
                clarity_delta=clarity_delta,
                immunity_delta=immunity_delta,
                efficiency_delta=efficiency_delta,
                autonomy_delta=autonomy_delta
            )

            # Burden naturally decreases as phase regime improves
            # create_demo_burden() automatically provides phase-appropriate burden levels
            burden = create_demo_burden(state.phase_regime)

            system.capture_snapshot(state, burden, include_advanced_analysis=False)

            if state.phase_regime not in phase_transitions:
                phase_transitions.append(state.phase_regime)

        # Update z_actual with final achieved value
        z_actual = state.z_coordinate

        # Get final snapshot with predicted burden reduction
        final_snapshot = system.snapshots[-1]
        final_weighted = final_snapshot.weighted_burden

        # Use the system's predicted burden reduction (from BurdenReductionCalculator)
        # This is the theoretically-predicted reduction based on cascade mechanics
        burden_reduction_pct = final_snapshot.predicted_burden_reduction

        # Also track actual observed reduction for comparison
        actual_reduction_pct = ((initial_weighted - final_weighted) / initial_weighted) * 100 if initial_weighted > 0 else 0

        # Measure cascade amplification
        # Get R1, R2, R3 from final state
        r1 = state.clarity  # CORE
        r2 = state.immunity  # BRIDGES
        r3 = state.efficiency  # META

        r1_to_r2 = r2 / r1 if r1 > 0 else 0
        r2_to_r3 = r3 / r2 if r2 > 0 else 0
        cascade_amp = r3 / r1 if r1 > 0 else 0

        return TrialResult(
            trial_id=trial_id,
            z_target=z_target,
            z_actual=z_actual,
            initial_burden=initial_weighted,
            final_burden=final_weighted,
            burden_reduction_pct=burden_reduction_pct,
            cascade_amplification=cascade_amp,
            r1_to_r2_ratio=r1_to_r2,
            r2_to_r3_ratio=r2_to_r3,
            snapshots_count=len(system.snapshots),
            phase_transitions=phase_transitions
        )

    def run_z_range_scan(self) -> List[ZRangeAnalysis]:
        """
        Systematically scan z-range with multiple trials per point.

        Returns:
            List of ZRangeAnalysis objects, one per z-value tested
        """
        z_analyses = []

        z_current = self.z_min
        trial_counter = 0

        print(f"Starting z-range scan: [{self.z_min}, {self.z_max}] step {self.z_step}")
        print(f"Trials per z-value: {self.trials_per_z}")
        print(f"Total trials: {int((self.z_max - self.z_min) / self.z_step + 1) * self.trials_per_z}\n")

        while z_current <= self.z_max + 1e-9:  # Small epsilon for floating point
            print(f"Testing z = {z_current:.3f}...")

            # Run multiple trials at this z-value
            z_trials = []
            for trial_num in range(self.trials_per_z):
                trial = self.run_single_trial(z_current, trial_counter)
                z_trials.append(trial)
                self.trial_results.append(trial)
                trial_counter += 1

                print(f"  Trial {trial_num + 1}/{self.trials_per_z}: "
                      f"reduction={trial.burden_reduction_pct:.1f}%, "
                      f"amp={trial.cascade_amplification:.2f}")

            # Compute statistics for this z-value
            reductions = [t.burden_reduction_pct for t in z_trials]
            amplifications = [t.cascade_amplification for t in z_trials]

            mean_red = statistics.mean(reductions)
            std_red = statistics.stdev(reductions) if len(reductions) > 1 else 0

            # Compute z-score relative to predicted 15.3%
            if std_red > 0:
                z_score = (mean_red - self.predicted_reduction) / std_red
            else:
                z_score = 0

            # Compute p-value (two-tailed test)
            # Using normal approximation
            p_value = self._compute_p_value(z_score)

            # 95% confidence interval
            ci_margin = 1.96 * std_red / math.sqrt(len(reductions))
            ci_95 = (mean_red - ci_margin, mean_red + ci_margin)

            # Coefficient of variation (CV)
            cv = (std_red / mean_red * 100) if mean_red > 0 else 0

            analysis = ZRangeAnalysis(
                z_value=z_current,
                trials_count=len(z_trials),
                mean_reduction=mean_red,
                std_reduction=std_red,
                min_reduction=min(reductions),
                max_reduction=max(reductions),
                coefficient_variation=cv,
                z_score=z_score,
                p_value=p_value,
                confidence_interval_95=ci_95,
                is_significant=(p_value < 0.001),
                mean_amplification=statistics.mean(amplifications),
                std_amplification=statistics.stdev(amplifications) if len(amplifications) > 1 else 0
            )

            z_analyses.append(analysis)

            print(f"  ‚Üí Mean reduction: {mean_red:.1f}% ¬± {std_red:.1f}%")
            print(f"  ‚Üí z-score: {z_score:.2f}, p-value: {p_value:.6f}")
            print(f"  ‚Üí Significant: {analysis.is_significant}\n")

            z_current += self.z_step

        return z_analyses

    def generate_report(self, z_analyses: List[ZRangeAnalysis]) -> ReproducibilityReport:
        """
        Generate comprehensive reproducibility report.

        Args:
            z_analyses: List of per-z statistical analyses

        Returns:
            ReproducibilityReport with findings and recommendations
        """
        # Overall statistics
        all_reductions = [t.burden_reduction_pct for t in self.trial_results]
        overall_mean = statistics.mean(all_reductions)
        overall_std = statistics.stdev(all_reductions)
        overall_z_score = (overall_mean - self.predicted_reduction) / overall_std if overall_std > 0 else 0
        overall_p_value = self._compute_p_value(overall_z_score)

        # Find optimal z-value
        optimal_analysis = max(z_analyses, key=lambda a: a.mean_reduction)

        # Determine if reproducible
        is_reproducible = (
            overall_p_value < 0.001 and  # Highly significant
            overall_mean > 50 and  # Substantial reduction
            all(a.coefficient_variation < 20 for a in z_analyses)  # Stable across z
        )

        # Detect emergence vs artifact
        # Emergence: consistent high performance across z-range
        # Artifact: high variance, isolated peaks, unstable
        emergence_score = sum(1 for a in z_analyses if a.mean_reduction > 50)
        emergence_detected = emergence_score > len(z_analyses) * 0.7  # 70%+ show high performance

        artifact_score = sum(1 for a in z_analyses if a.coefficient_variation > 30)
        artifact_suspected = artifact_score > len(z_analyses) * 0.3  # 30%+ show high variance

        # Generate recommendations
        recommendations = []

        if is_reproducible and emergence_detected:
            recommendations.append(
                "‚úÖ VALIDATED: Burden reduction is reproducible and likely represents "
                "real emergent behavior. Proceed with Garden Rail 3 development."
            )
        elif is_reproducible and not emergence_detected:
            recommendations.append(
                "‚ö†Ô∏è  CAUTION: Reproducible but isolated to narrow z-range. "
                "Consider targeted deployment at optimal z-value."
            )
        elif artifact_suspected:
            recommendations.append(
                "‚ùå ARTIFACT SUSPECTED: High variance suggests measurement issues. "
                "Investigate burden calculation methodology before proceeding."
            )
        else:
            recommendations.append(
                "üî¨ INCONCLUSIVE: Results suggest real effect but need more data. "
                "Extend study with n=50 trials per z-value."
            )

        # Additional recommendations
        if optimal_analysis.z_value != 0.867:
            recommendations.append(
                f"üí° INSIGHT: Peak performance at z={optimal_analysis.z_value:.3f}, "
                f"not z=0.867. Update critical point model."
            )

        if any(a.mean_amplification < 3.0 for a in z_analyses):
            recommendations.append(
                "‚ö†Ô∏è  WARNING: Cascade amplification below 3.0√ó in some regions. "
                "Verify R1‚ÜíR2‚ÜíR3 coupling strength."
            )

        report = ReproducibilityReport(
            study_id=f"REPRO_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            timestamp=datetime.now().isoformat(),
            z_range=(self.z_min, self.z_max),
            z_step=self.z_step,
            trials_per_z=self.trials_per_z,
            total_trials=len(self.trial_results),
            predicted_reduction=self.predicted_reduction,
            overall_mean_reduction=overall_mean,
            overall_std_reduction=overall_std,
            overall_z_score=overall_z_score,
            overall_p_value=overall_p_value,
            z_analyses=z_analyses,
            is_reproducible=is_reproducible,
            optimal_z_value=optimal_analysis.z_value,
            optimal_reduction=optimal_analysis.mean_reduction,
            emergence_detected=emergence_detected,
            artifact_suspected=artifact_suspected,
            recommendations=recommendations
        )

        return report

    def export_report(self, report: ReproducibilityReport, filepath: str):
        """
        Export report to JSON file.

        Args:
            report: ReproducibilityReport to export
            filepath: Output file path
        """
        # Convert dataclass to dict
        report_dict = asdict(report)

        with open(filepath, 'w') as f:
            json.dump(report_dict, f, indent=2)

        print(f"Report exported to: {filepath}")

    def print_summary(self, report: ReproducibilityReport):
        """Print human-readable summary of report."""
        print("\n" + "=" * 80)
        print("REPRODUCIBILITY STUDY SUMMARY")
        print("=" * 80)
        print(f"Study ID: {report.study_id}")
        print(f"Timestamp: {report.timestamp}")
        print(f"Z-range: [{report.z_range[0]}, {report.z_range[1]}] (step: {report.z_step})")
        print(f"Total trials: {report.total_trials}")
        print()

        print("OVERALL RESULTS")
        print("-" * 80)
        print(f"Predicted reduction: {report.predicted_reduction}%")
        print(f"Observed reduction:  {report.overall_mean_reduction:.1f}% ¬± {report.overall_std_reduction:.1f}%")
        print(f"Overperformance:     {report.overall_mean_reduction - report.predicted_reduction:.1f}%")
        print(f"Z-score:             {report.overall_z_score:.2f}")
        print(f"P-value:             {report.overall_p_value:.2e}")
        print(f"Reproducible:        {'‚úÖ YES' if report.is_reproducible else '‚ùå NO'}")
        print(f"Emergence detected:  {'‚úÖ YES' if report.emergence_detected else '‚ùå NO'}")
        print(f"Artifact suspected:  {'‚ö†Ô∏è  YES' if report.artifact_suspected else '‚úÖ NO'}")
        print()

        print("OPTIMAL PERFORMANCE")
        print("-" * 80)
        print(f"Optimal z-value:     {report.optimal_z_value:.3f}")
        print(f"Reduction at peak:   {report.optimal_reduction:.1f}%")
        print()

        print("RECOMMENDATIONS")
        print("-" * 80)
        for i, rec in enumerate(report.recommendations, 1):
            print(f"{i}. {rec}")
        print()

        print("PER-Z ANALYSIS")
        print("-" * 80)
        print(f"{'z-value':<10} {'Mean%':<10} {'Std%':<10} {'CV%':<10} {'z-score':<10} {'Sig?':<8}")
        print("-" * 80)
        for analysis in report.z_analyses:
            sig = "‚úì" if analysis.is_significant else "‚úó"
            print(f"{analysis.z_value:<10.3f} "
                  f"{analysis.mean_reduction:<10.1f} "
                  f"{analysis.std_reduction:<10.1f} "
                  f"{analysis.coefficient_variation:<10.1f} "
                  f"{analysis.z_score:<10.2f} "
                  f"{sig:<8}")
        print("=" * 80 + "\n")

    def _compute_weighted_burden(self, burden: BurdenMeasurement) -> float:
        """Compute weighted burden score."""
        return (
            burden.coordination * 1.0 +
            burden.decision_making * 1.2 +
            burden.context_switching * 0.9 +
            burden.maintenance * 0.8 +
            burden.learning_curve * 1.1 +
            burden.emotional_labor * 1.3 +
            burden.uncertainty * 1.5 +
            burden.repetition * 0.7
        ) / 8.5

    def _compute_p_value(self, z_score: float) -> float:
        """
        Compute two-tailed p-value from z-score using normal approximation.

        Args:
            z_score: Standard score

        Returns:
            Two-tailed p-value
        """
        # Using complementary error function approximation
        # For large |z|, use asymptotic approximation to avoid underflow
        abs_z = abs(z_score)

        if abs_z > 8:
            # For very large z-scores, p-value is effectively 0
            return 1e-15

        # Standard normal CDF approximation
        t = 1.0 / (1.0 + 0.2316419 * abs_z)
        d = 0.3989423 * math.exp(-z_score * z_score / 2.0)
        prob = d * t * (0.3193815 + t * (-0.3565638 + t * (1.781478 + t * (-1.821256 + t * 1.330274))))

        # Two-tailed
        return 2.0 * prob


def main():
    """Run reproducibility study."""
    print("TRIAD 084 - Reproducibility Study")
    print("Testing z-coordinate range [0.85, 0.90] for burden reduction validation\n")

    # Initialize study
    study = ReproducibilityStudy(
        z_min=0.85,
        z_max=0.90,
        z_step=0.01,
        trials_per_z=20  # 20 trials per z-value for statistical power
    )

    # Run z-range scan
    z_analyses = study.run_z_range_scan()

    # Generate report
    report = study.generate_report(z_analyses)

    # Print summary
    study.print_summary(report)

    # Export to JSON
    report_path = f"reproducibility_report_{report.study_id}.json"
    study.export_report(report, report_path)

    return 0 if report.is_reproducible else 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
